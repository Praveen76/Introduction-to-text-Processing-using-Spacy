{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveen76/Introduction-to-text-Processing-using-Spacy/blob/main/Introduction-to-text-Processing-using-Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the spaCy library\n",
        "* perform simple natural language processing tasks using the spaCy library"
      ],
      "metadata": {
        "id": "akVZjBjD2PtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "VGel018GxQGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spaCy** is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "It is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
        "\n",
        "spaCy's features and capabilities include:\n",
        "\n",
        "- ***Tokenization***:\tSegmenting text into words, punctuations marks etc.\n",
        "- ***Part-of-speech (POS) Tagging***: Assigning word types to tokens, like verb or noun.\n",
        "- ***Dependency Parsing***: Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
        "- ***Lemmatization***: Assigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
        "- ***Sentence Boundary Detection (SBD)***: Finding and segmenting individual sentences.\n",
        "- ***Named Entity Recognition (NER)***: Labelling named “real-world” objects, like persons, companies or locations.\n",
        "- ***Entity Linking (EL)***: Disambiguating textual entities to unique identifiers in a knowledge base.\n",
        "- ***Similarity***: Comparing words, text spans and documents and how similar they are to each other.\n",
        "- ***Text Classification***: Assigning categories or labels to a whole document, or parts of a document.\n",
        "- ***Rule-based Matching***: Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
        "- ***Training***: Updating and improving a statistical model's predictions.\n",
        "- ***Serialization***: Saving objects to files or byte strings.\n"
      ],
      "metadata": {
        "id": "TI_iPJtzxSEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical models\n",
        "\n",
        "While some of spaCy's features work independently, others require ***trained pipelines*** to be loaded, which enable spaCy to predict linguistic annotations - for example, whether a word is a verb or a noun.\n",
        "\n",
        "A trained pipeline can consist of multiple components that use a statistical model trained on labeled data.\n",
        "\n",
        "spaCy currently offers trained pipelines for a variety of languages, which can be installed as individual Python modules. Pipeline packages can differ in size, speed, memory usage, accuracy and the data they include.\n",
        "\n",
        "For English language, available trained pipelines include:\n",
        "- `en_core_web_sm`\n",
        "- `en_core_web_md`\n",
        "- `en_core_web_lg`\n",
        "- `en_core_web_trf` - English transformer pipeline\n",
        "\n",
        "To know more about trained pipelines for English, refer [here](https://spacy.io/models/en).\n",
        "\n",
        "Let's perform basic NLP tasks with spaCy using an English trained pipeline.\n"
      ],
      "metadata": {
        "id": "k9SWBYOydg-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install packages"
      ],
      "metadata": {
        "id": "ysivBdlXMxoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy==3.7.4"
      ],
      "metadata": {
        "id": "RnmSjKExJ-AK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlHYHxmhJWqC",
        "outputId": "5fbe2aca-01ee-44bb-bfc1-1bd3b9df816f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.7.4                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-6.1.58+-x86_64-with-glibc2.35\n",
            "Python version   3.10.12                       \n",
            "Pipelines        en_core_web_sm (3.7.1)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above info, we can see that by default spaCy contains the small trained pipeline for English `en_core_web_sm`.\n",
        "\n",
        "To use medium, large, and transformer trained pipelines, they need to be installed first using the `!python -m spacy download` command.\n",
        "\n",
        "For example: `!python -m spacy download en_core_web_trf`"
      ],
      "metadata": {
        "id": "ioo8J8kk270t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install English transformer pipeline\n",
        "# NOTE that Runtime needs to restart after this step\n",
        "\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "enIOTr8y6hf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b053821-4cfa-4395-cf85-4ed5581fff1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-trf==3.7.3\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.7.3) (3.7.4)\n",
            "Collecting spacy-curated-transformers<0.3.0,>=0.2.0 (from en-core-web-trf==3.7.3)\n",
            "  Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.25.2)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.2.1+cu121)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.10/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2023.12.25)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, curated-tokenizers, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, curated-transformers, spacy-curated-transformers, en-core-web-trf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Runtime/Session**"
      ],
      "metadata": {
        "id": "HpB5KKWSKkby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "id": "x83z3TOfNGS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "d6wPl_vwqL6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFOiNNeP0giN"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the trained pipeline\n",
        "\n",
        "Once you've downloaded and installed a trained pipeline, you can load it via `spacy.load()`. This will return a *Language object* containing all components and data needed to process text. We usually call it `nlp`.\n"
      ],
      "metadata": {
        "id": "OoRmCXB55X3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transformer pipeline for English\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# This gives us a Language object\n",
        "nlp"
      ],
      "metadata": {
        "id": "GGlQY8Ro2hw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esentially, spaCy's *Language* object is a pipeline that uses the language model to perform a number of natural language processing tasks such as *tokenization*, *part-of-speech tagging*, *syntactic parsing*, *named entity recognition*, etc.\n",
        "\n",
        "<br>\n",
        "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/spacy_pipeline.png' width=800px>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "UeGDyVaDpczn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing basic NLP tasks using spaCy"
      ],
      "metadata": {
        "id": "cVztM7aSHg-N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWwX8KlNK3NX"
      },
      "source": [
        "Calling the Language object, `nlp`, on a string of text will return a processed *Doc*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLEKXAxxK3NX"
      },
      "outputs": [],
      "source": [
        "# An example sentence\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beDKSs4XK3NY"
      },
      "outputs": [],
      "source": [
        "# Feed the string object under 'text' to the Language object under 'nlp'\n",
        "# Store the result under the variable 'doc'\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "id": "-d28toejq29Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xDMhpziK3NX"
      },
      "source": [
        "Passing the variable `text` to the _Language_ object `nlp` returns a spaCy *Doc* object, short for document.\n",
        "\n",
        "This object contains both the input text stored under `text` and the results of natural language processing using spaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69rWTI8oK3NY"
      },
      "outputs": [],
      "source": [
        "# Call the variable to examine the object\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLU_RXJuK3NZ"
      },
      "source": [
        "Calling the variable `doc` returns the contents of the object.\n",
        "\n",
        "Although the output resembles that of a Python string, the *Doc* object contains a wealth of information about its linguistic structure, which spaCy generated by passing the text through the NLP pipeline.\n",
        "\n",
        "Let's examine the tasks that were performed under the hood after the input sentence was provided to the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhcEc6AiK3NZ"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybAUeP7dK3Na"
      },
      "source": [
        "*Tokenization* breaks the text down into words, punctuation and so on.\n",
        "\n",
        "The diagram below outlines the tasks that spaCy can perform after a text has been tokenised, such as *part-of-speech tagging*, *syntactic parsing* and *named entity recognition*.\n",
        "\n",
        "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/spacy_pipeline.png' width=800px>\n",
        "\n",
        "Each *Doc* consists of individual tokens, and we can iterate over them.\n",
        "\n",
        "Let's print out each *Token* object stored in the _Doc_ object `doc`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxUEyrg8K3Nb"
      },
      "outputs": [],
      "source": [
        "# Tokens present inside the document\n",
        "\n",
        "print(\"Token\\n\"+'='*20)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KJaYn-3K3Nc"
      },
      "source": [
        "### Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnGIsQRHK3Nd"
      },
      "source": [
        "Part-of-speech (POS) tagging is the task of determining the word class of a token. This is crucial for *disambiguation*, because different parts of speech may have similar forms.\n",
        "\n",
        ">Consider the example: *The sailor dogs the hatch*.<br>\n",
        ">The present tense of the verb *dog* (to fasten something with something) is precisely the same as the plural form of the noun *dog*: *dogs*.\n",
        "\n",
        "To identify the correct word class, we must examine the context in which the word appears.\n",
        "\n",
        "spaCy provides two types of part-of-speech tags, coarse and fine-grained, which are stored under the attributes `pos_` and `tag_`, respectively.\n",
        "\n",
        "To access the results of POS tagging, let's loop over the *Doc* object `doc` and print each *Token* and its part-of-speech tags."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the token and the POS tags\n",
        "\n",
        "print(f\"{' ':<30}POS tag\\n{' ':<20}{'-'*25}\")\n",
        "print(f\"{'Token':<20}{'Coarse':<13}Fine-grained\\n{'='*45}\")\n",
        "\n",
        "for token in doc:\n",
        "    coarse = token.pos_         # coarse pos tag\n",
        "    fine = token.tag_           # fine-grained pos tag\n",
        "\n",
        "    print(f\"{token.text:<20}{coarse:<13}{fine}\")"
      ],
      "metadata": {
        "id": "Va5tmZcItRvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8u4RTfgK3Ny"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcxwD63HK3Nz"
      },
      "source": [
        "A **lemma** is the base form of a word.\n",
        "\n",
        "Unless explicitly instructed, computers cannot tell the difference between singular and plural forms of words, but treat them as distinct tokens, because their forms differ.\n",
        "\n",
        "For instance, if we want to count the occurrences of words, a process known as _lemmatization_ is needed to group together the different forms of the same token.\n",
        "\n",
        "Lemmas are available for each _Token_ under the attribute `lemma_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtMeYKd4K3Nz"
      },
      "outputs": [],
      "source": [
        "# Print the token and its base form\n",
        "\n",
        "print(f\"{'Token':<20} Lemma\\n{'='*30}\")\n",
        "\n",
        "for token in doc:\n",
        "    lemma = token.lemma_\n",
        "    print(f\"{token.text:<20} {lemma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyeCdI6iK3Nz"
      },
      "source": [
        "### Named entity recognition (NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv2U9VEzK3N0"
      },
      "source": [
        "Named entity recognition (NER) is the task of recognising and classifying entities named in a text.\n",
        "\n",
        "spaCy can recognise the named entities such as persons, geographic locations, and products as these were annotated in the dataset its trained on (OntoNotes 5 corpus).\n",
        "\n",
        "We can use the *Doc* object's `.ents` attribute to get the named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wn523SiK3N0"
      },
      "outputs": [],
      "source": [
        "# Entities\n",
        "doc.ents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This returns a tuple with the named entities.\n",
        "\n",
        "Each item in the tuple is a spaCy *Span* object. *Span* objects can consist of multiple *Token* objects, as many named entities span multiple *Tokens*."
      ],
      "metadata": {
        "id": "oM4MeyX2qlas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the type of the object used to store named entities\n",
        "type(doc.ents[0])"
      ],
      "metadata": {
        "id": "NGs7oHNjqMGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaCzY90CK3N0"
      },
      "source": [
        "The named entities and their types are stored under the attributes `.text` and `.label_` of each *Span* object.\n",
        "\n",
        "Let's loop over the *Span* objects in the tuple and print out both attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yveeIWxXK3N0"
      },
      "outputs": [],
      "source": [
        "# Loop over the named entities in the Doc object, and print the named entity and its label\n",
        "\n",
        "print(f\"{'Text':<20} {'Entity_label':<16} Explanation\\n{'='*80}\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    ent_text = ent.text           # named entity\n",
        "    ent_label = ent.label_        # entity label\n",
        "    ent_label_val = spacy.explain(ent_label)       # entity label explanation\n",
        "\n",
        "    print(f\"{ent_text:<20} {ent_label:<16} {ent_label_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GCgHkJ7K3N1"
      },
      "source": [
        "As you can see, named entities like '$1 billion' identified in the *Doc* consist of multiple *Tokens*, which is why they are represented as *Span* objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGjkG8m3K3N2"
      },
      "source": [
        "spaCy [*Span*](https://spacy.io/api/span) objects contain several useful arguments.\n",
        "\n",
        "Most importantly, the attributes `start` and `end` return the indices of _Tokens_, which determine where the _Span_ starts and ends in the *Doc* object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPwNfX-VK3N2"
      },
      "outputs": [],
      "source": [
        "# Print the named entity and indices of its start and end Tokens\n",
        "print(doc.ents[2], doc.ents[2].start, doc.ents[2].end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z29gu0lwK3N3"
      },
      "source": [
        "The named entity starts at index 8 and ends at index 11 in the *Doc* object."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize Named Entities"
      ],
      "metadata": {
        "id": "3ycEVv9KrNkZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTKE_TnYK3N4"
      },
      "source": [
        "We can also render the named entities using *displacy*, the spaCy module we used for visualising dependency parses above.\n",
        "\n",
        "Note that we must pass the string `ent` to the `style` argument to indicate that we wish to visualise named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUN9_a2KK3N5"
      },
      "outputs": [],
      "source": [
        "# Visualize named entity\n",
        "spacy.displacy.render(doc, style='ent')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test another example 2:**"
      ],
      "metadata": {
        "id": "I5fLOBFpxSt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize another sample text\n",
        "text2 = \"On 3rd Feb, Ram was in Delhi.\\nLater he traveled to Mumbai via Air India flight reading a Time magazine to meet Raj.\\nAfter 10 days, he went again back to Delhi wearing a Timex watch.\"\n",
        "doc2 = nlp(text2)\n",
        "spacy.displacy.render(doc2, style='ent')"
      ],
      "metadata": {
        "id": "kZLngerFvbL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a particular tag used for a named entity is unfamiliar, you can check it's explanation."
      ],
      "metadata": {
        "id": "ogdrSb382R3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('DATE')"
      ],
      "metadata": {
        "id": "D-dFic41xMda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('PERSON')"
      ],
      "metadata": {
        "id": "IBp1zyMXxnJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test another example 3:**"
      ],
      "metadata": {
        "id": "XAvWA3882Ues"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize another sample text\n",
        "text3 = \"Holmes solves his another case while sitting at his home in Baker Street, without moving a single inch.\"\n",
        "doc3 = nlp(text3)\n",
        "spacy.displacy.render(doc3, style='ent')"
      ],
      "metadata": {
        "id": "UV6e_njh1LKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('FAC')"
      ],
      "metadata": {
        "id": "L76wtVpb1n6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "* https://spacy.io/usage/spacy-101"
      ],
      "metadata": {
        "id": "YFXGM68ARzhb"
      }
    }
  ]
}